# All Resources

## 3 Nvidia DGX H100 nodes (dgxh-[1-3])

Processor 	 2x 56-core 2.0 Ghz Intel Xeon Platinum 8480CL CPU with 105 MB cache
Memory 	 2.0 TB RAM @4800 MT/s

## 6 Nvidia DGX2 nodes (dgx2-[1-6])

Processor 	 2x 24-core 2.70 Ghz Intel Xeon Platinum 8168 CPU with 33792 KB cache
Memory 	 1.5 TB RAM @2666 MT/s

## 3 Nvidia DGXS nodes (dgxs-[1-3])

Processor 	 20-core 2.20 Ghz Intel Xeon E5-2698 with 51200 KB cache
Memory 	 256 GB RAM @2400 MT/s

## 22 AMD EPYC compute nodes:

### cn-v-[1-8]

Processor 	 2x 48-core 3.6 GHz AMD EPYC 9474F w/ 512 MB cache
Memory 	 384 GB RAM @4800 MT/s
**NO GPUs**

### cn-u-[1-2]
Processor 	 2x 32-core 2.8 GHz AMD EPYC 7543 w/ 32768 KB cache
Memory 	 512 GB RAM @3200 MT/s
**NO GPUs**

### cn-t-1
Processor 	 2x 24-core 3.2 GHz AMD EPYC 7313 w/ 32768 KB cache
Memory 	 256 GB RAM @3200 MT/s

### cn-s-[1-5]
Processor 	 2x 24-core 3.2 GHz AMD EPYC 74F3 w/ 32768 KB cache
Memory 	 256 GB RAM @3200 MT/s

### cn-r-[5-6]
Processor 	 2x 32-core 2.6 GHz AMD EPYC 7513 w/ 32768 KB cache
Memory 	 256 GB RAM @3200 MT/s

### cn-r-[1-4]
Processor 	 2x 24-core 3.2 GHz AMD EPYC 7F72 w/ 16384 KB cache
Memory 	 256 GB RAM @3200 MT/s

## 44 Skylake compute nodes:

### sail-gpu0
Processor 	 2x 24-core 3.0 GHz Intel Xeon Gold 6248R w/ 36608 KB cache
Memory 	 768 GB RAM @2933 MT/s

### soundwave
Processor 	 2x 10-core 2.40 GHz Intel Xeon Silver 4210R w/ 14080 KB cache
Memory 	 384 GB RAM @2666 MT/s

### optimus
Processor 	 2x 20-core 2.10 GHz Intel Xeon Gold 6230 w/ 28160 KB cache
Memory 	 384 GB RAM @2933 MT/s

### cn-gpu[6-7]
Processor 	 2x 24-core 3.0 GHz Intel Xeon Gold 6248R w/ 36608 KB cache
Memory 	 768 GB RAM @2933 MT/s

### cn-gpu5
Processor 	 2x 20-core 2.50 GHz Intel Xeon Gold 6248 w/ 28160 KB cache
Memory 	 768 GB RAM @2933 MT/s

### cn-q-[1-2]
Processor 	 2x 24-core 3.0 GHz Intel Xeon Gold 6248R w/ 36608 KB cache
Memory 	 192 GB RAM @2933 MT/s
**NO GPUs**

### cn-p-1
Processor 	 2x 24-core 3.0 GHz Intel Xeon Gold 6254R w/ 36608 KB cache
Memory 	 384 GB RAM @2933 MT/s

### cn-o-1​​​​
Processor 	 2x 24-core 3.0 GHz Intel Xeon Gold 6254R w/ 36608 KB cache
Memory 	 192 GB RAM @2933 MT/s
**NO GPUs**

### cn-n-[1-6]
Processor 	 2x 18-core 3.10 GHz Intel Xeon Gold 6254 w/ 25344 KB cache
Memory 	 192 GB RAM @2933 MT/s
**NO GPUs**

### cn-m-1
Processor 	 2x 4-core 3.80 GHz Intel Xeon Gold 5222 w/ 16896 KB cache
Memory 	 192 GB RAM @2933 MT/s

### cn-m-2
Processor 	 2x 8-core 2.50 GHz Intel Xeon Silver 4215 w/ 11264 KB cache
Memory 	 192 GB RAM @2666 MT/s

### cn-k-[1-4], cn-l-[1-2]
Processor 	 2x 10-core 2.20 GHz Intel Xeon Silver 4114 w/ 14080 KB cache
Memory 	 64 GB RAM @2666 MT/s
**NO GPUs**

### cn-j-[1-4]
Processor 	 2x 16-core 2.10 GHz Intel Xeon Gold 6130 w/ 22528 KB cache
Memory 	 512 GB RAM @2666 MT/s
**NO GPUs**

### cn-i-[4-5]
Processor 	 2x 12-core 2.60 GHz Intel Xeon Gold 6126 w/ 19712 KB cache
Memory 	 256 GB RAM @2666 MT/s
**NO GPUs**

### cn-i-[1-3]
Processor 	 2x 12-core 2.10 GHz Intel Xeon Silver 4116 w/ 16896 KB cache
Memory 	 128 GB RAM @2666 MT/s
**NO GPUs**

### cn-h-[5-8]
Processor 	 2x 22-core 2.10 GHz Intel Xeon Gold 6152 w/ 30976 KB cache
Memory 	 128 GB RAM @2666 MT/s
**NO GPUs**

### cn-h-[1-4]
Processor 	 2x 10-core 2.20 GHz Intel Xeon Silver 4114 w/ 16896 KB cache
Memory 	 128 GB RAM @2666 MT/s
**NO GPUs**

### cn-g-[1-4]
Processor 	 2x 14-core 2.20 GHz Intel Xeon Gold 5120 w/ 19712 KB cache
Memory 	 64-128 GB RAM @2666 MT/s
**NO GPUs**

## 21 Dell Intel Haswell/Broadwell compute nodes:

### cn-gpu0
Processor 	 2x 12-core 2.30 GHz Intel Xeon E5-2670v3 w/ 30720 KB cache
Memory 	 256 GB RAM @2133 MT/s

### cn-f-[1-5]
Processor 	 2x 10-core 2.60 GHz Intel Xeon E5-2660v3 w/ 25600 KB cache
Memory 	 128 GB RAM @2133 MT/s
**NO GPUs**

### cn-e-[1-8]
Processor 	 2x 10-core 2.60 GHz Intel Xeon E5-2660v3 w/ 25600 KB cache
Memory 	 64-128 GB RAM @2133 MT/s
**NO GPUs**

### cn-d-[1-2]
Processor 	 2x 14-core 2.00 GHz Intel Xeon E7-4830v4 w/ 35840 KB cache
Memory 	 384 GB RAM @2400 MT/s
**NO GPUs**

### cn-d-[3-5]
Processor 	 2x 20-core 2.10 GHz Intel Xeon E7-8870v4 w/ 51200 KB cache
Memory 	 512-1536 GB RAM @2400 MT/s
**NO GPUs**

### cn-c-[7-8]
Processor 	 2x 8-core 2.40 GHz Intel Xeon E5-2630v3 w/ 20480 KB cache
Memory 	 128-256 GB RAM @1866 MT/s
**NO GPUs**

## 25 Dell Intel Sandy-bridge/Ivy-bridge compute nodes:

### cn-b-[1-6], cn-c-[1-6]
Processor 	 2x 8-core 2.60 GHz Intel Xeon E5-2650v2 w/ 20480 KB cache
Memory 	 64-96 GB RAM @1866 MT/s
**NO GPUs**

### cn-a-[3-10]
Processor 	 2x 8-core 2.90 GHz Intel Xeon E5-2690 w/ 20480 KB cache
Memory 	 256 GB RAM @1600 MT/s
**NO GPUs**

### cn-a-[1-2]
Processor 	 2x 8-core 2.60 GHz Intel Xeon E5-2670 w/ 20480 KB cache
Memory 	 128 GB RAM @1600 MT/s
**NO GPUs**

### cn-9-[1-4]
Processor 	 2x 6-core 2.50 GHz Intel Xeon E5-2640 w/ 15360 KB cache
Memory 	 128 GB RAM @1333 MT/s
**NO GPUs**

# Greater than 3.0 GHz Compute
**NOTE:** I should look into NUMA domains and whether these compute nodes have those setup. If they do, then jobs might not be able to use resources across the 2x processors. (For example, if there are 2x48 cores, and I request 15 cores, is it that I have to use 15 cores from one of those 2x processors, or that I can use some cores from the first processor, and some cores from the second?)
**NOTE:** How much ram do I ask for vs how much ram do I need? Is it possible that if I reduce the requested RAM then I can run more experiments on these nodes?
**NOTE:** Maybe I can play around with runtimes on my laptop to see if I can reduce requested cores faster?
**NOTE:** Sam recommends not using the Xeon processors (similar to low tier laptop processor)*
**NOTE:** Cache is a like a way faster, more expensive version of RAM. Makes operations way faster because getting/putting things on the cache is much faster than getting them from RAM.

## 22 AMD EPYC compute nodes:

### cn-v-[1-8]

Processor 	 2x 48-core 3.6 GHz AMD EPYC 9474F w/ 512 MB cache
Memory 	 384 GB RAM @4800 MT/s
Partitions=nacse, preempt
**NO GPUs**

### cn-t-1
Processor 	 2x 24-core 3.2 GHz AMD EPYC 7313 w/ 32768 KB cache
Memory 	 256 GB RAM @3200 MT/s
Partitions=athena,preempt

### cn-s-[1-5]
Processor 	 2x 24-core 3.2 GHz AMD EPYC 74F3 w/ 32768 KB cache
Memory 	 256 GB RAM @3200 MT/s
[1-4] Partitions=class,nacse,preempt
5 Partitions=class,athena,nacse,preempt

### cn-r-[1-4]
Processor 	 2x 24-core 3.2 GHz AMD EPYC 7F72 w/ 16384 KB cache
Memory 	 256 GB RAM @3200 MT/s
Partitions=share,class,ampere,preempt

## 44 Skylake compute nodes:

### cn-n-[1-6]
Processor 	 2x 18-core 3.1 GHz Intel Xeon Gold 6254 w/ 25344 KB cache
Memory 	 192 GB RAM @2933 MT/s
**NO GPUs**

### cn-m-1
Processor 	 2x 4-core 3.8 GHz Intel Xeon Gold 5222 w/ 16896 KB cache
Memory 	 192 GB RAM @2933 MT/s
**NOTE:** I request 15 cores with each job, so I can't request this unless I configure my jobs to use less cores.

# Optimization

submit-b ~/influence-shaping/sbatch/01_05_2025/australia 1059$ squeue -w cn-u-2 -o "%.18i %.16P %.90j %.8u %.2t %.10M %.6D %R"
             JOBID        PARTITION                                                                                       NAME     USER ST       TIME  NODES NODELIST(REASON)
          18229624          preempt            01_05_2025.australia.standard_four_squares.D-Indirect-Timestep-Difference.t1.sh gonzaeve  R   10:59:18      1 cn-u-2
          18229625          preempt            01_05_2025.australia.standard_four_squares.D-Indirect-Timestep-Difference.t2.sh gonzaeve  R   10:53:16      1 cn-u-2
          18229626          preempt            01_05_2025.australia.standard_four_squares.D-Indirect-Timestep-Difference.t3.sh gonzaeve  R   10:52:48      1 cn-u-2
          18229629          preempt                                    01_05_2025.australia.standard_four_squares.Global.t1.sh gonzaeve  R   10:50:48      1 cn-u-2
submit-b ~/influence-shaping/sbatch/01_05_2025/australia 1060$ cat slurm-18229624.out
 28%|██▊       | 277/1000 [10:56:46<28:49:24, 143.52s/it]submit-b ~/influence-shaping/sbatch/01_05_2025/australia 1061$ cat slurm-18229625.out
 27%|██▋       | 272/1000 [10:49:05<29:14:44, 144.62s/it]submit-b ~/influence-shaping/sbatch/01_05_2025/australia 1062$ cat slurm-18229626.out
 28%|██▊       | 280/1000 [10:50:34<28:06:09, 140.51s/it]submit-b ~/influence-shaping/sbatch/01_05_2025/australia 1063$ cat slurm-18229629.out
 82%|████████▏ | 817/1000 [10:50:03<2:27:32, 48.37s/it]submit-b ~/influence-shaping/sbatch/01_05_2025/australia 1064$ 

 Time to optimize... D-Indirect-Timestep-Difference takes WAYYYY too long
 

# Partition Information

What I have used so far: 
share,dgx2,ampere,preempt,mime1

These are the accounts (not partitions) that I have access to.
sacctmgr show associations user=gonzaeve format=Account,Partition,QOS

   Account  Partition                  QOS 
---------- ---------- -------------------- 
    kt-lab                          normal 
      mime                          normal

I have access to kt-lab and mime.

## preempt: 
AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
AllocNodes=ALL Default=NO QoS=N/A
Nodes=cn-a[01,04,10-16,20-27],cn-b[01-12],cn-c[10-17,20-24,30-33],cn-d[01,11-13,21],cn-e[01-15,21-24,31-34,41-44],cn-gpu[1-7],cn-m-[1-2],cn-n-[1-6],cn-o-1,cn-p-1,cn-q-[1-2],cn-r-[1-6],cn-s-[1-5],cn-t-1,cn-u-[1-2],cn-v-[1-8],cn-w-1,dgx2-[1-6],dgxh-[1-4],optimus,sail-gpu0,soundwave
**PriorityJobFactor=1 PriorityTier=1** RootOnly=NO ReqResv=NO OverSubscribe=NO
OverTimeLimit=NONE **PreemptMode=REQUEUE**

Noticing that the priorityjobfactor and prioritytier on preempt are both 1... are other partitions higher? How high and in what order? What partitions actually have the power to preempt these jobs?

## share:
AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
AllocNodes=ALL Default=YES QoS=share
Nodes=cn-a[10-15,20-27],cn-b[04-09],cn-c[30-33],cn-d21,cn-e[03-06,31-34,41-42],cn-gpu1,cn-r-[1-6]
**PriorityJobFactor=1 PriorityTier=2** RootOnly=NO ReqResv=NO OverSubscribe=NO
OverTimeLimit=NONE **PreemptMode=OFF**

## nacse:
AllowGroups=ALL AllowAccounts=nacse,coeit AllowQos=ALL
**PriorityJobFactor=1 PriorityTier=4** RootOnly=NO ReqResv=NO OverSubscribe=NO
Nodes=cn-n-[1-6],cn-s-[1-5],cn-v-[1-8]
OverTimeLimit=NONE **PreemptMode=OFF**

I don't think I am allowed to use this partition. It may belong to the Northwest Alliance for Computational Science & Engineering.

Since PreemptMode is OFF, does that means that if I run jobs on nodes of nacse using the preempt partition, that my jobs won't actually get superseded by nacse jobs? I think that is what that means. So preempt doesn't always get preempted, it depends on the configuration of the partition that would be canceling the preempt job.

## athena
AllowGroups=ALL AllowAccounts=hlab,coeit AllowQos=ALL
**PriorityJobFactor=1 PriorityTier=4** RootOnly=NO ReqResv=NO OverSubscribe=NO
Nodes=cn-s-5,cn-t-1
OverTimeLimit=NONE **PreemptMode=OFF**

## class
AllowGroups=ALL AllowAccounts=ai530,ai535,ai537,ai539,aae462-562,che542,cs462,cs479-579,cs499-599,cs535,cs539,ece468,mats588,me373,me499-599,me540,nse451-551,nse565,coeit AllowQos=ALL
Nodes=cn-r-[1-6],cn-s-[1-5]
**PriorityJobFactor=1 PriorityTier=4** RootOnly=NO ReqResv=NO OverSubscribe=NO
OverTimeLimit=NONE **PreemptMode=OFF**

## ampere
AllowGroups=ALL AllowAccounts=ALL AllowQos=ALL
AllocNodes=ALL Default=NO QoS=ampere
Nodes=cn-r-[1-6]
**PriorityJobFactor=1 PriorityTier=3** RootOnly=NO ReqResv=NO OverSubscribe=NO
OverTimeLimit=NONE **PreemptMode=OFF**

64 MaxCPUsPerNode for ampere compared to 32 MaxCPUsPerNode for share. Also higher priority tier.

## Questions:

How do PriorityJobFactor and PriorityTier work? Especially if preempt mode is off... Then are these parameters only important for resolving ties when queuing up what job comes next or something like that?

I think I was wrong about how I initially thought PreemptMode works. It's not about whether this job preempts other jobs, it's about what a job running on this partition does if it is preempted.

What is QoS? I think stands for Quality of Service, but if QoS=ampere, then what does that even mean?

I sometimes get a QOSMinGRES error when submitting jobs. The strange thing is that eventually the jobs do end up submitting. But I think I understand why I am getting this error and why my jobs end up submitting anyways.

A QOSMinGRES means that my job was submitted to a QOS (Quality of Service) that enforces a minimum GRES requirement, but my job does not request the required GRES. So I think that for some subset of the partitions that I submitted to, I do not meet the requirements to run on the nodes in that partition. For example, I might need to request more memory or more CPUs or a GPU in order to be allowed to run on a particular partition.

But because I specify multiple partitions in sbatch, my job just ends up going to a different partition where I do meet the minimum GRES.

My jobs currently are running in preempt and share. Even though I also specified ampere as well. I bet I don't meet the quality of service requirements for ampere, otherwise my jobs would have run on ampere.

It seems like I need to specify the required GRES using
#SBATCH --gres=

I used
sacctmgr show qos ampere

to try to figure out what the quality of service issue. Under MinTRES I see gres/gpu=1. I think in order to use the ampere partition I need to request a gpu. I don't need a gpu (at least at this point in my experiments) and requesting one would mean I can't run my experiments on some of the cn-v nodes, so I should probably just take ampere out of my sbatch files. I don't meet the quality of service requirements and reconfiguring my jobs to meet those requirements would actually prevent me from running jobs on the nodes I really want to run them on. (Also, I don't need or even use a gpu, so it would just be taking up resources.)


I may be able to use --prefer to specify that I would like to run my jobs on certain nodes, but if they are not available, then run my jobs elsewhere. But I don't think I can specify nodes with --prefer, it seems like I have to specify features. But what I can do is specify features unique to the nodes I want to run on.. and I think sbatch will also include the constraints... Not sure exactly how constraints and prefer interplay with each other.

Oh it seems like maybe I can specify I want cpus that have a certain clock speed with this option
--cpu-freq
There's also this bit in the slurm docs:
NOTE: When submitting jobs with the --cpu-freq option with linuxproc as the ProctrackType can cause jobs to run too quickly before Accounting is able to poll for job information. As a result not all of accounting information will be present.

This option to spread a job across many nodes is interesting. I don't think it would work well for me though because it would mean I would have to request a ton of resources and somehow specify how many corse I need in each node... or something like that. But maybe worth considering this exists for future usage.
--spread-job
    Spread the job allocation over as many nodes as possible and attempt to evenly distribute tasks across the allocated nodes. This option disables the topology/tree plugin. 

